{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "- Training NN with a large data is slow. So to find an optimization algorithm that runs faster is a good idea.\n",
    "- Suppose we have m = 50 million. To train this data it will take a huge processing time for one step.\n",
    "   - because 50 million won't fit in the memory at once we need other processing to make such a thing.\n",
    "- It turns out you can make a faster algorithm to make gradient descent process some of your items even before you finish the 50 million items.\n",
    "- Suppose we have split m to mini batches of size 1000.\n",
    "  - X{1} = 0 ... 1000\n",
    "  - X{2} = 1001 ... 2000\n",
    "  - ....\n",
    "  - X{bs} = ...\n",
    "- We similarly split X & Y.\n",
    "- So the definition of mini batches ==> t: X{t}, Y{t}\n",
    "- In Batch gradient descent we run the gradient descent on the whole dataset.\n",
    "- While in Mini-Batch gradient descent we run the gradient descent on the mini datasets.\n",
    "- Mini-Batch algorithm pseudo code:\n",
    "ss\n",
    "- The code inside an epoch should be vectorized.\n",
    "- Mini-batch gradient descent works much faster in the large datasets.\n",
    "\n",
    "### Understanding mini-batch gradient descent\n",
    "- In mini-batch algorithm, the cost wont go down with each step as it does in batch algorithm. It could contain some ups and downs but generally it has to go down (unlike the batch gradient descent where cost function descreases on each iteration).\n",
    "- Mini-batch size:\n",
    "   - (mini batch size = m) ==> Batch gradient descent\n",
    "   - (mini batch size = 1) ==> Stochastic gradient descent (SGD)\n",
    "   - (mini batch size = between 1 and m) ==> Mini-batch gradient descent\n",
    "- Batch gradient descent:\n",
    "  - too long per iteration (epoch)\n",
    "- Stochastic gradient descent:\n",
    "    - too noisy regarding cost minimization (can be reduced by using smaller learning rate)\n",
    "    - won't ever converge (reach the minimum cost)\n",
    "    - lose speedup from vectorization\n",
    "- Mini-batch gradient descent:\n",
    "   1. faster learning:\n",
    "        - you have the vectorization advantage\n",
    "        - make progress without waiting to process the entire training set\n",
    "   2. doesn't always exactly converge (oscelates in a very small region, but you can reduce learning rate)\n",
    "- Mini-batch size is a hyperparameter.\n",
    "- Guidelines for choosing mini-batch size:\n",
    "   1. If small training set (< 2000 examples) - use batch gradient descent.\n",
    "   2. It has to be a power of 2: (because of the way computer memory is layed out and accessed, sometimes your code runs faster if your mini-batch size is a power of 2): 64, 128, 256, 512, 1024, ...\n",
    "   3. Make sure that mini-batch fits in CPU/GPU memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "- Vectorization allows you to efficiently compute on m examples.\n",
    "- Training NN with a large data is slow. So to find an optimization algorithm that runs faster is a good idea.\n",
    "- Suppose we have m = 50 million. To train this data it will take a huge processing time for one step.\n",
    "   - because 50 million won't fit in the memory at once we need other processing to make such a thing.\n",
    "- It turns out you can make a faster algorithm to make gradient descent process some of your items even before you finish the 50 million items.\n",
    "- little baby training sets and these baby training sets are called mini-batches\n",
    "- Suppose we have split m to mini batches of size 1000.\n",
    "  - X{1} = 0 ... 1000\n",
    "  - X{2} = 1001 ... 2000\n",
    "  - ....\n",
    "  - X{bs} = ...\n",
    "- We similarly split X & Y.\n",
    "- So the definition of mini batches ==> t: X{t}, Y{t}\n",
    "- In Batch gradient descent we run the gradient descent on the whole dataset.\n",
    "- While in Mini-Batch gradient descent we run the gradient descent on the mini datasets.\n",
    "- Mini-Batch algorithm pseudo code:\n",
    "<img src=\"img/Screen%20Shot%202019-02-01%20at%2022.33.14.png\">\n",
    "- The code inside an epoch should be vectorized.\n",
    "- Mini-batch gradient descent works much faster in the large datasets.\n",
    "\n",
    "### Understanding mini-batch gradient descent\n",
    "- In mini-batch algorithm, the cost wont go down with each step as it does in batch algorithm. It could contain some ups and downs but generally it has to go down (unlike the batch gradient descent where cost function descreases on each iteration).\n",
    "- Mini-batch size:\n",
    "   - (mini batch size = m) ==> Batch gradient descent\n",
    "   - (mini batch size = 1) ==> Stochastic gradient descent (SGD)\n",
    "   - (mini batch size = between 1 and m) ==> Mini-batch gradient descent\n",
    "- Batch gradient descent:\n",
    "  - too long per iteration (epoch)\n",
    "- Stochastic gradient descent:\n",
    "    - too noisy regarding cost minimization (can be reduced by using smaller learning rate)\n",
    "    - won't ever converge (reach the minimum cost)\n",
    "    - lose speedup from vectorization\n",
    "- Mini-batch gradient descent:\n",
    "   1. faster learning:\n",
    "        - you have the vectorization advantage\n",
    "        - make progress without waiting to process the entire training set\n",
    "   2. doesn't always exactly converge (oscelates in a very small region, but you can reduce learning rate)\n",
    "- Mini-batch size is a hyperparameter.\n",
    "- Guidelines for choosing mini-batch size:\n",
    "   1. If small training set (< 2000 examples) - use batch gradient descent.\n",
    "   2. It has to be a power of 2: (because of the way computer memory is layed out and accessed, sometimes your code runs faster if your mini-batch size is a power of 2): 64, 128, 256, 512, 1024, ...\n",
    "   3. Make sure that mini-batch fits in CPU/GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentially weighted averages\n",
    "\n",
    "Optimization algorithms are faster than gradient descent. \n",
    "General equation: V(t) = beta * v(t-1) + (1-beta) * theta(t)\n",
    "If we plot this it will represent averages over ~ (1 / (1 - beta)) entries:\n",
    "   - beta = 0.9 will average last 10 entries\n",
    "   - beta = 0.98 will average last 50 entries  (1/(1-0.98))\n",
    "   - beta = 0.5 will average last 2 entries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding exponentially weighted averages\n",
    "Algorithm is very simple:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "v = 0\n",
    "Repeat\n",
    "{\n",
    "     Get next theta(t)\n",
    "     v = beta * v + (1-beta) * theta(t)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias correction in exponentially weighted averages\n",
    "- The bias correction helps make the exponentially weighted averages more accurate.\n",
    "- v(t) = (beta * v(t-1) + (1-beta) * theta(t)) / (1 - beta^t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with momentum\n",
    "- The momentum algorithm almost always works faster than standard gradient descent.\n",
    "- The simple idea is to calculate the exponentially weighted averages for your gradients and then update your weights with the new values.\n",
    "- Pseudo code:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vdW = 0, vdb = 0\n",
    "on iteration t:\n",
    "\t# can be mini-batch or batch gradient descent\n",
    "\tcompute dw, db on current mini-batch                \n",
    "\t\t\t\n",
    "\tvdW = beta * vdW + (1 - beta) * dW\n",
    "\tvdb = beta * vdb + (1 - beta) * db\n",
    "\tW = W - learning_rate * vdW\n",
    "\tb = b - learning_rate * vdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Momentum helps the cost function to go to the minimum point in a more fast and consistent way\n",
    "- beta is another hyperparameter. beta = 0.9 is very common and works very well in most cases.\n",
    "- In practice people don't bother implementing bias correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "This algorithm speeds up the gradient descent."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sdW = 0, sdb = 0\n",
    "on iteration t:\n",
    "\t# can be mini-batch or batch gradient descent\n",
    "\tcompute dw, db on current mini-batch\n",
    "   \n",
    "   sdW = (beta * sdW) + (1 - beta) * dW^2  # squaring is element-wise\n",
    "\tsdb = (beta * sdb) + (1 - beta) * db^2  # squaring is element-wise\n",
    "\tW = W - learning_rate * dW / sqrt(sdW)\n",
    "\tb = B - learning_rate * db / sqrt(sdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSprop will make the cost function move slower on the vertical direction(b) and faster on the horizontal direction (w)\n",
    "- Ensure that sdW is not zero by adding a small value epsilon (e.g. epsilon = 10^-8) to it:\n",
    "    -  W = W - learning_rate * dW / (sqrt(sdW) + epsilon)\n",
    "- With RMSprop you can increase your learning rate.\n",
    "\n",
    "### Adam optimization algorithm\n",
    "- Stands for Adaptive Moment Estimation.\n",
    "- Adam optimization simply puts RMSprop and momentum together!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vdW = 0, vdW = 0\n",
    "sdW = 0, sdb = 0\n",
    "on iteration t:\n",
    "    # can be mini-batch or batch gradient descent\n",
    "    compute dw, db on current mini-batch \n",
    "    vdW = (beta1 * vdW) + (1 - beta1) * dW     # momentum\n",
    "    vdb = (beta1 * vdb) + (1 - beta1) * db     # momentum\n",
    "    \n",
    "    sdW = (beta2 * sdW) + (1 - beta2) * dW^2   # RMSprop\n",
    "\t sdb = (beta2 * sdb) + (1 - beta2) * db^2   # RMSprop\n",
    "    \n",
    "    vdW = vdW / (1 - beta1^t)      # fixing bias\n",
    "    vdb = vdb / (1 - beta1^t)      # fixing bias\n",
    "    \n",
    "    sdW = sdW / (1 - beta2^t)      # fixing bias\n",
    "    sdb = sdb / (1 - beta2^t)      # fixing bias\n",
    "    \n",
    "    W = W - learning_rate * vdW / (sqrt(sdW) + epsilon)\n",
    "\t b = B - learning_rate * vdb / (sqrt(sdb) + epsilon)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameters for Adam:\n",
    "     - Learning rate: needed to be tuned.\n",
    "     - beta1: parameter of the momentum - 0.9 is recommended by default.\n",
    "     - beta2: parameter of the RMSprop - 0.999 is recommended by default.\n",
    "     - epsilon: 10^-8 is recommended by default.\n",
    "\n",
    "### Learning rate decay\n",
    "- Slowly reduce learning rate.\n",
    "- One technique equations islearning_rate = (1 / (1 + decay_rate * epoch_num)) * learning_rate_0\n",
    "- epoch_num is over all data (not a single mini-batch).\n",
    "- Other learning rate decay methods (continuous):\n",
    "     - learning_rate = (0.95 ^ epoch_num) * learning_rate_0\n",
    "     - learning_rate = (k / sqrt(epoch_num)) * learning_rate_0\n",
    "- Some people are making changes to the learning rate manually.\n",
    "- decay_rate is another hyperparameter.\n",
    "- For Andrew Ng, learning rate decay has less priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

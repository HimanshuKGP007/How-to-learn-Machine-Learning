{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep L-layer neural network\n",
    "\n",
    "- Shallow NN is a NN with one or two layers.\n",
    "- Deep NN is a NN with three or more layers.\n",
    "- We will use the notation L to denote the number of layers in a NN\n",
    "- n[l] is the number of neurons in a specific layer l.\n",
    "- n[0] denotes the number of neurons input layer.\n",
    "- g[l] is the activation function.\n",
    "- a[l] = g[l](z[l])\n",
    "- w[l] weights is used for z[l]\n",
    "- x = a[0], a[l]= y'\n",
    "- we have:\n",
    "  - A vector n of shape(1, NoOfLayers+1)\n",
    "  - A vector g of shape(1, NoOfLayers)\n",
    "  - A list of different shapes w based on the number of neurons on the previous and the current layer.\n",
    "  - A list of different shapes b based on the number of neurons on the current layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation in a Deep Network\n",
    "#### Layer 1\n",
    "\n",
    "<img src=\"img/Screen%20Shot%202019-01-19%20at%2017.46.30.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layer2\n",
    "- Z[2] =  W[2]a[i] + b[2]\n",
    "- a[2] = g[2] (z[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation general rule to one input:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- z[l] = W[l] a[l-1] + b [l]  # a0 = x\n",
    "- a[l] = g[l] (z[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation general rule for m inputs:\n",
    "- for l = 1..4\n",
    "- Z[l] = W[l]A[l-1] + B[l]\n",
    "- A[l] = g[l] (A[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your matrix dimensions right\n",
    "- Dimension of w is (n[l], n[l-1]). Can thought by right to left.\n",
    "- Dimension of b is (n[l], 1)\n",
    "- dw has the same shape as w, while db is the same shape as b\n",
    "- Dimension of Z[l], A[l], dZ[l], and dA[l] is (n[l],m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why deep representations?\n",
    "- Deep NN makes relations with data from simpler to complex. in each layer it tries to make a relation with the previous layer. Eg:\n",
    "   - Face recogntion application:\n",
    "     - Image => Edges => Face parts => Faces => desired face\n",
    "   - Audio recogntion application:\n",
    "     - Audio => Low level sound features like (sss, bb) => Phonemes => Words => Sentences\n",
    "- Neural Researchers think that deep neural networks \"think\" like brains (simple ==> complex)\n",
    "- When starting on an application don't start directly by dozens of hidden layers. Try the simplest solution (e.g Logistic Regression), then try the shallow neural network and so on.\n",
    "\n",
    "Edge detection is an image processing technique for finding the boundaries of objects within images. It works by detecting discontinuities in brightness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks of deep neutral networks\n",
    "- forward propagation and back propagation, the key components you need to implement a deep NN\n",
    "<img src=\"img/Screen%20Shot%202019-01-19%20at%2023.13.23.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and backward funcitons\n",
    "\n",
    "<img src=\"img/Screen%20Shot%202019-01-19%20at%2023.40.20.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and Backward \n",
    "- Forward propagation for layer l:\n",
    "- Input A[l-1]\n",
    "- Z[l] = W[l]A[l-1] + b[l]\n",
    "- A[l] = g[l] (Z[l])\n",
    "- Output A[l], cache(Z[l])\n",
    "\n",
    "- Back propagation for layer l:\n",
    "- Input da[l], Caches\n",
    "- dZ[l] = dA[l] * g'[l] (Z[l])\n",
    "- dW[l] = (dZ[l]A[l-1].T) / m\n",
    "- db[l] = sum(dZ[l]) / m   # Dont forget axis = 1, keepdims = True\n",
    "- dA[l-1] = w[l].T * dZ[l]  # The multiplication here are a dot product\n",
    "- Output dA[l-1], dW[l], db[l]\n",
    "\n",
    "- If we have used our loss function then:\n",
    "- dA[l] = (-(y/a) + ((1-y)/(1-a)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

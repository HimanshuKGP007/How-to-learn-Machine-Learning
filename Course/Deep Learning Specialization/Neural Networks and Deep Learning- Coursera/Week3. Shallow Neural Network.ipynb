{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Overview\n",
    "- In logistic regression we had:\n",
    "<img src=\"img/Screen%20Shot%202019-01-16%20at%201.03.00.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- In neural networks with one layer we will have:\n",
    "<img src=\"img/Screen%20Shot%202019-01-16%20at%201.32.49.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral Network Representation\n",
    "<img src=\"img/Screen%20Shot%202019-01-16%20at%201.41.13.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a0 = x (the input layer)\n",
    "- a1 will represent the activation of the hidden neurons.\n",
    "- a2 will represent the output layer.\n",
    "\n",
    "### Computing a Neural Network's Output\n",
    "\n",
    "<img src=\"img/Screen%20Shot%202019-01-16%20at%2023.25.08.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here are some informations about the last image:\n",
    "  - noOfHiddenNeurons = 4\n",
    "  - Nx = 3\n",
    "  - Shapes of the variables:\n",
    "    - W1 is the matrix of the first hidden layer, it has a shape of (noOfHiddenNeurons,nx)\n",
    "    - b1 is the matrix of the first hidden layer, it has a shape of (noOfHiddenNeurons,1)\n",
    "    - z1 is the result of the equation z1 = W1*X + b, it has a shape of (noOfHiddenNeurons,1)\n",
    "    - a1 is the result of the equation a1 = sigmoid(z1), it has a shape of (noOfHiddenNeurons,1)\n",
    "    - W2 is the matrix of the second hidden layer, it has a shape of (1,noOfHiddenNeurons)\n",
    "    - b2 is the matrix of the second hidden layer, it has a shape of (1,1)\n",
    "    - z2 is the result of the equation z2 = W2*a1 + b, it has a shape of (1,1)\n",
    "    - a2 is the result of the equation a2 = sigmoid(z2), it has a shape of (1,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing across multiple examples\n",
    "- Pseudo code for forward propagation for the 2 layers NN:\n",
    "<img src=\"img/Screen%20Shot%202019-01-17%20at%2022.08.52.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets say we have X on shape (Nx,m). So the new pseudo code:\n",
    "<img src=\"img/Screen%20Shot%202019-01-17%20at%2022.30.12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you notice always m is the number of columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "- So far we are using sigmoid, but in some case other function can be a lot better.\n",
    "- Tanh activation function range is [-1, 1] \n",
    "<img src=\"img/Screen%20Shot%202019-01-17%20at%2023.02.08.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or A = np.tanh(z) # where z is the input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sigmoid or Tanh function disadvantage is that if the input is too small or too high, the slope will be near zero which will cause us the gradient decent problem.\n",
    "- So here is some basic rule for choosing activation functions, if your classification is between 0 and 1, use the output activation as sigmoid and the others as RELU.\n",
    "- The advantage of both the ReLU and the leaky ReLU is that for a lot of the space of Z, the derivative of the activation function, the slope is very different from 0. \n",
    "- So using the ReLU activation function, your NN will often learn much faster than when using the tanh or the sigmoid activation function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do you need non-linear activation functions?\n",
    "- Whatever hidden layers you add, the activation will be always linear like logistic regression (So its useless in a lot of complex problems)\n",
    "- You might use linear activation function in one place - in the output layer if the output is real numbers (regression problem). But even in this case if the output value is non-negative you could use RELU instead.\n",
    "\n",
    "housing price boston\n",
    "<img src=\"img/Screen%20Shot%202019-01-18%20at%200.42.33.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives of activation functions\n",
    "- Derivation of Sigmoid activation function:\n",
    "<img src=\"img/Screen%20Shot%202019-01-18%20at%201.07.31.png\">\n",
    "\n",
    "- Derivation of Tanh activation function:\n",
    "<img src=\"img/Screen%20Shot%202019-01-18%20at%201.08.10.png\">\n",
    "\n",
    "- Derivation of RELU activation function:\n",
    "<img src=\"img/Screen%20Shot%202019-01-18%20at%201.09.37.png\">\n",
    "- Derivation of leaky RELU activation function:\n",
    "<img src=\"img/Screen%20Shot%202019-01-18%20at%201.10.07.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient desceent for neural networks\n",
    "<img src=\"img/Screen%20Shot%202019-01-18%20at%201.30.21.png\">\n",
    "\n",
    "db[2] = 1/m * np.sum(dZ[2], axis =1, keepdims = True) => (n, 1)\n",
    "- axis = 1: summing hozirontally\n",
    "- keepdims: it prevents Python from outputting one of those funny rrank one arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation:\n",
    "- Z1 = W1A0 + b1 #A0 is X\n",
    "- A1 = g1(Z1)\n",
    "- Z2 = W2A1 + b2\n",
    "- A2 = Sigmoid(Z2) # Sigmoid because the output is between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Back propagation (derivations): \n",
    "   - dZ2 = A2 - Y #derivative of cost function we used * derivative of the sigmoid function\n",
    "   - dW2 = (dZ2 * A1.T) / m\n",
    "   - db2 = Sum(dZ2) / m\n",
    "   - dZ1 = (W2.T * dZ2) * g'1(Z1)  # element wise product (*)\n",
    "   - dW1 = (dZ1 * A0.T) / m   # A0 = X\n",
    "   - db1 = Sum(dZ1) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization\n",
    "\n",
    "- In logistic regression it wasn't important to initialize the weights randomly, while in NN we have to initialize them randomly.\n",
    "- If we initialize all the weights with zeros in NN it won't work (initializing bias with zero is OK):\n",
    "  - all hidden units will be completely identical (symmetric) - compute exactly the same function (a1 = a2)\n",
    "  - on each gradient descent iteration all the hidden units will always update the same\n",
    "- To solve this we initialize the W's with a small random numbers:\n",
    "    - W1 = np.random.randn((2,2)) * 0.01    # 0.01 to make it small enough\n",
    "    - b1 = np.zeros((2,1))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

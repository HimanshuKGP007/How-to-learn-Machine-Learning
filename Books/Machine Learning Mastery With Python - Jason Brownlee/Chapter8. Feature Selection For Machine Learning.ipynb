{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Date: 2018/11/01"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After completing this lesson you will know how to use:\n1. Univariate Selection.\n2. Recursive Feature Elimination. \n3. Principle Component Analysis. \n4. Feature Importance.\n\n### 8.1 Feature Selection\n\nFeature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested. \nThree benefits of performing feature selection before modeling your data are:\n- Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n- Improves Accuracy: Less misleading data means modeling accuracy improves.\n- Reduces Training Time: Less data means that algorithms train faster."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 8.2 Univariate Selection"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Statistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The example below uses the chi-squared (chi2) statistical test for non-negative features to select 4 of the best features from the Pima Indians onset of diabetes dataset."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\nimport pandas\nimport numpy\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# load data\nurl = \"http://ftp.ics.uci.edu/pub/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\nnames=[\"preg\", \"plas\", \"pres\", \"skin\", \"test\", \"mass\", \"pedi\", \"age\", \"class\"] \ndataframe = pandas.read_csv(url, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\n# feature extraction\ntest = SelectKBest(score_func=chi2, k=4)\nfit = test.fit(X, Y)\n# summarize scores\nnumpy.set_printoptions(precision=3)\nprint(fit.scores_)\nfeatures = fit.transform(X)\n# summarize selected features\nprint(features[0:5,:])",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n[[148.    0.   33.6  50. ]\n [ 85.    0.   26.6  31. ]\n [183.    0.   23.3  32. ]\n [ 89.   94.   28.1  21. ]\n [137.  168.   43.1  33. ]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores): plas, test, mass and age."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 8.3 Recursive Feature Elimination\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain. \nIt uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The example below uses RFE with the logistic regression algorithm to select the top 3 features. The choice of algorithm does not matter too much as long as it is skillful and consistent."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Feature Extraction with RFE\nfrom pandas import read_csv\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n# load data\nurl = \"http://ftp.ics.uci.edu/pub/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\nnames=[\"preg\", \"plas\", \"pres\", \"skin\", \"test\", \"mass\", \"pedi\", \"age\", \"class\"] \ndataframe = read_csv(url, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 3)\nfit = rfe.fit(X, Y)\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Num Features: 3\nSelected Features: [ True False False False False  True  True False]\nFeature Ranking: [1 2 3 5 6 1 1 4]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can see that RFE chose the top 3 features as preg, mass and pedi. These are marked True in the support array and marked with a choice 1 in the ranking array. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 8.4 Principal Component Analysis\n\nPrincipal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form. \nGenerally this is called a data reduction technique. \nA property of PCA is that you can choose the number of dimensions or principal components in the transformed result. In the example below, we use PCA and select 3 principal components. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Feature Extraction with PCA\nimport numpy\nfrom pandas import read_csv\nfrom sklearn.decomposition import PCA\n# load data\nurl = \"http://ftp.ics.uci.edu/pub/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\nnames=[\"preg\", \"plas\", \"pres\", \"skin\", \"test\", \"mass\", \"pedi\", \"age\", \"class\"] \ndataframe = read_csv(url, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\n# feature extraction\npca = PCA(n_components=3)\nfit = pca.fit(X)\n# summarize components\nprint(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\nprint(fit.components_)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Explained Variance: [0.889 0.062 0.026]\n[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n   5.372e-04 -3.565e-03]\n [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n  -8.168e-04 -1.402e-01]\n [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n  -6.400e-04 -1.255e-01]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 8.5 Feature Importance\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Feature Importance with Extra Trees Classifier\nfrom pandas import read_csv\nfrom sklearn.ensemble import ExtraTreesClassifier\n# load data\nurl = \"http://ftp.ics.uci.edu/pub/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\nnames=[\"preg\", \"plas\", \"pres\", \"skin\", \"test\", \"mass\", \"pedi\", \"age\", \"class\"] \ndataframe = read_csv(url, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\n# feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, Y)\nprint(model.feature_importances_)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[0.119 0.234 0.1   0.081 0.08  0.122 0.117 0.148]\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can see that we are given an importance score for each attribute where the larger the score, the more important the attribute. The scores suggest at the importance of plas, age and mass."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}